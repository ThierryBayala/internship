{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: en\n",
      "Vocabulary size: 57852\n",
      "\n",
      "finished getting raw counts\n",
      "closest match by euclidean distance: ontario\n",
      "meningitis - i = ontario - vaccine\n",
      "\n",
      "closest match by cosine distance: ontario\n",
      "meningitis - i = ontario - vaccine\n",
      "\n",
      "closest match by euclidean distance: ruled\n",
      "meningitis - meningitis = ruled - vaccine\n",
      "\n",
      "closest match by cosine distance: swine\n",
      "meningitis - meningitis = swine - vaccine\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from flask import Flask\n",
    "from cleaner import *\n",
    "from kafka import KafkaConsumer\n",
    "from util import *\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, KernelPCA\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def main(text):\n",
    "    analogies_to_try = (\n",
    "        ('meningitis', 'i', 'vaccine'),\n",
    "        ('meningitis', 'canadian', 'vaccine')\n",
    "      \n",
    "  )\n",
    "\n",
    "    sentences, word2idx = get_data(text.strip(),n_vocab=100)\n",
    "\n",
    "    notfound = False\n",
    "    for word_list in analogies_to_try:\n",
    "        for w in word_list:\n",
    "            if w not in word2idx:\n",
    "                print(\"%s not found in vocab, remove it from \\\n",
    "                    analogies to try or increase vocab size\")\n",
    "                notfound = True\n",
    "    if notfound:\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # build term document matrix\n",
    "    V = len(word2idx)\n",
    "    N = len(sentences)\n",
    "\n",
    "    # create raw counts first\n",
    "    A = np.zeros((V, N))\n",
    "    j = 0\n",
    "    for sentence in sentences:\n",
    "        for i in sentence:\n",
    "            A[i,j] += 1\n",
    "        j += 1\n",
    "    print(\"finished getting raw counts\")\n",
    "\n",
    "    transformer = TfidfTransformer()\n",
    "    A = transformer.fit_transform(A.T).T\n",
    "\n",
    "    # tsne requires a dense array\n",
    "    A = A.toarray()\n",
    "\n",
    "    # map back to word in plot\n",
    "    idx2word = {v:k for k, v in iteritems(word2idx)}\n",
    "\n",
    "    # plot the data in 2-D\n",
    "    tsne = TSNE()\n",
    "    Z = tsne.fit_transform(A)\n",
    "    plt.scatter(Z[:,0], Z[:,1])\n",
    "    for i in range(V):\n",
    "        try:\n",
    "            plt.annotate(s=idx2word[i].encode(\"utf8\").decode(\"utf8\"), xy=(Z[i,0], Z[i,1]))\n",
    "        except:\n",
    "            print(\"bad string:\", idx2word[i])\n",
    "    plt.draw()\n",
    "\n",
    "    ### multiple ways to create vectors for each word ###\n",
    "    # 1) simply set it to the TF-IDF matrix\n",
    "    # We = A\n",
    "\n",
    "    # 2) create a higher-D word embedding\n",
    "    tsne = TSNE(n_components=3)\n",
    "    We = tsne.fit_transform(A)\n",
    "\n",
    "    # 3) use a classic dimensionality reduction technique\n",
    "    # svd = KernelPCA(n_components=20, kernel='rbf')\n",
    "    # We = svd.fit_transform(A)\n",
    "\n",
    "    for word_list in analogies_to_try:\n",
    "        w1, w2, w3 = word_list\n",
    "        find_analogies(w1, w2, w3, We, word2idx, idx2word)\n",
    "        \n",
    "\n",
    "    plt.show() # pause script until plot is closed\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text=''\n",
    "    parsed_topic_name = 'corpus_tweet'\n",
    "    consumer = KafkaConsumer(parsed_topic_name, auto_offset_reset='earliest',\n",
    "                             bootstrap_servers=['localhost:9092'], api_version=(0, 10), consumer_timeout_ms=1000)\n",
    "    for msg in consumer:\n",
    "        record=json.loads(msg.value)\n",
    "        text=record['tweet']\n",
    "    \n",
    "    main(text)\n",
    "    \n",
    "    sleep(3)\n",
    "\n",
    "    if consumer is not None:\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
